{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8049b45",
   "metadata": {},
   "source": [
    "# The tensorfow implementation of the Word2Vec paper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a60f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce5a3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7968a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae1f508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "    \n",
    "    #Build the sampling table for vocab_size tokens\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    \n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        \n",
    "        # Generate positive skip=gram pairs for a squences (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence, \n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "        \n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with a positive context word and negative samples\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"),1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0) \n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bef5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b54fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1f95ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dea888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:'\n",
      "b'Before we proceed any further, hear me speak.'\n",
      "b'All:'\n",
      "b'Speak, speak.'\n",
      "b'First Citizen:'\n"
     ]
    }
   ],
   "source": [
    "# print elements from the dataset to see empty lines have been removed\n",
    "# The b's printed in the lines below signify\n",
    "# bytes = b'...' literals = a sequence of octets (integers between 0 and 255)\n",
    "i = 0\n",
    "# print(text_ds.as_numpy_iterator())\n",
    "for elem in text_ds.as_numpy_iterator():\n",
    "    print(elem)\n",
    "    i+=1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17f8b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom standardization function  to lowercase the text and remove punctuation\n",
    "def custom_standardization(input_data):\n",
    "    input_lowercased = tf.strings.lower(input_data)\n",
    "\n",
    "    # Escape special characters in pattern. \n",
    "    # and then use regex_replace to replace them with '' in the input string\n",
    "    return tf.strings.regex_replace(input_lowercased, \n",
    "                                   '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence \n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the 'TextVectorization'  layer to normalize, split and map string to\n",
    "# integers. Set the 'output_sequence_length' lentght \n",
    "# to pad all samples to the same length\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e4ba520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call TextVectorization.adapt  on the text dataset to create vocabulary\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1336cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your', 'his', 'this', 'but', 'he', 'have', 'as', 'thou', 'him', 'so', 'what', 'thy', 'will', 'no', 'by', 'all', 'king', 'we', 'shall', 'her', 'if']\n"
     ]
    }
   ],
   "source": [
    "#  Save the created vocabulary for reference \n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(type(inverse_vocab))\n",
    "print(inverse_vocab[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea67a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset shapes: (10,), types: tf.int64>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the data in text_ds\n",
    "text_vector_ds =  text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "text_vector_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a0c24",
   "metadata": {},
   "source": [
    "Print the first ten elements to see how the TextVectorization transformed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03e6a145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'First Citizen:'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Before we proceed any further, hear me speak.'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'All:'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Speak, speak.'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'First Citizen:'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'You are all resolved rather to die than to famish?'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'All:'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Resolved. resolved.'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'First Citizen:'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'First, you know Caius Marcius is chief enemy to the people.'>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_ds)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06ce076c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([138,  36, 982, 144, 673, 125,  16, 106,   0,   0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([34,  0,  0,  0,  0,  0,  0,  0,  0,  0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([106, 106,   0,   0,   0,   0,   0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([   7,   41,   34, 1286,  344,    4,  200,   64,    4, 3690])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([34,  0,  0,  0,  0,  0,  0,  0,  0,  0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1286, 1286,    0,    0,    0,    0,    0,    0,    0,    0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int64, numpy=array([  89,    7,   93, 1187,  225,   12, 2442,  592,    4,    2])>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_vector_ds)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b13bf",
   "metadata": {},
   "source": [
    "#### The multiplication of the target word with it's context using einsum\n",
    "<p>\n",
    "Here is a small example showing how the matrices are multiplied with einsum in the word2vec model.\n",
    "The structure shown here is similar to the tf.Data.Dataset that is fed into the Word2Vec Model\n",
    "just with lest data and smaller embedding dimension (embedding_dim==2).\n",
    "A (3,2) matrix 3 words each word has 2 dimensional embedding, \n",
    "we multiply each word with its context, meaning each word with the words from it's context\n",
    "and add the columns up into one column.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "930a2d76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_word = tf.constant([\n",
    "                 [1, 0.5],\n",
    "                 [1, 2],\n",
    "                 [1, 3.0]\n",
    "                 ], dtype=tf.float32)\n",
    "# 3,5,2 \n",
    "# 3 contexts \n",
    "# each context has 5 words \n",
    "# each word has 2 dimensional embedding \n",
    "context = tf.constant([\n",
    "                [[1, 0],\n",
    "                 [1, -2],\n",
    "                 [4, 2],\n",
    "                 [2, 2],\n",
    "                 [4,5]],\n",
    "                [[0, 2],\n",
    "                 [3, -1],\n",
    "                 [7, -2],\n",
    "                 [1, 2],\n",
    "                 [1,3]],\n",
    "                [[11, 2],\n",
    "                 [1, 12],\n",
    "                 [1, 5],\n",
    "                 [7, 6],\n",
    "                 [5,0]]\n",
    "                ],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4932f",
   "metadata": {},
   "source": [
    "#### Some more notes on Einsum - Einstein-Summation\n",
    "\n",
    "The firs matrix m1 you can see that each context is multiplied by the corresponding word for that context\n",
    "repeating letters in the einsum notation are multiplied\n",
    "and in the second matrix m2 you can see that the columns of those embedings are added up\n",
    "'be,bce->bc' because the 'e' does not appear in the result of the einsum, letters that appear in the first\n",
    "part of the einsum but not after the '->' are added up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b40866d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1: \n",
      " tf.Tensor(\n",
      "[[[ 1.   0. ]\n",
      "  [ 1.  -1. ]\n",
      "  [ 4.   1. ]\n",
      "  [ 2.   1. ]\n",
      "  [ 4.   2.5]]\n",
      "\n",
      " [[ 0.   4. ]\n",
      "  [ 3.  -2. ]\n",
      "  [ 7.  -4. ]\n",
      "  [ 1.   4. ]\n",
      "  [ 1.   6. ]]\n",
      "\n",
      " [[11.   6. ]\n",
      "  [ 1.  36. ]\n",
      "  [ 1.  15. ]\n",
      "  [ 7.  18. ]\n",
      "  [ 5.   0. ]]], shape=(3, 5, 2), dtype=float32)\n",
      "\n",
      " m2: \n",
      " tf.Tensor(\n",
      "[[ 1.   0.   5.   3.   6.5]\n",
      " [ 4.   1.   3.   5.   7. ]\n",
      " [17.  37.  16.  25.   5. ]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m1 = tf.einsum('be,bce->bce', target_word, context)\n",
    "m2 = tf.einsum('be,bce->bc', target_word, context) \n",
    "print('m1: \\n',m1)\n",
    "print('\\n m2: \\n',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "032abde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1. , 0.5]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_for_first_word = context[0]\n",
    "context_for_first_word_transposed = tf.transpose(context_for_first_word)\n",
    "target_word0_expanded = tf.expand_dims(target_word[0], axis=0)\n",
    "# tf.matmul(target[0],  m1context)\n",
    "# m1context\n",
    "target_word0_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dd37ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[ 1.,  1.,  4.,  2.,  4.],\n",
       "       [ 0., -2.,  2.,  2.,  5.]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_for_first_word_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a7f8d",
   "metadata": {},
   "source": [
    "####  The same multiplication as above but done with matrix multiplication and some extra explanation on word2vec\n",
    "You can see here that there is a matrix multiplication performed for each word embedding with its context\n",
    "and thus the new matrix is created\n",
    "and these are the final values in the word2vec model \n",
    "against these values the loss function is performed.\n",
    "First value in each row representing the true class and the rest the negative classes.\n",
    "In the backward pass the weights of the hidden layer are adjusted to minimize the  loss\n",
    "and these weights from the hidden layer represent the word embeddings.\n",
    "Note that the output here are some values that are larger than 1\n",
    "because values choosen in this example are also larger than 1 so it is easier to follow the calculations. \n",
    "Embeddings output by the embedding layer of keras will be between -0.05 and 0.05, because the RandomUniform initializer has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cab64f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[1. , 0. , 5. , 3. , 6.5]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[ 1. ,  0. ,  5. ,  3. ,  6.5],\n",
       "        [ 4. ,  1. ,  3. ,  5. ,  7. ],\n",
       "        [17. , 37. , 16. , 25. ,  5. ]], dtype=float32)>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = tf.matmul(target_word0_expanded, context_for_first_word_transposed)\n",
    "first_row, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e6f9451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bb56a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b961409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32777/32777 [00:17<00:00, 1883.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (65392,)\n",
      "contexts.shape: (65392, 5)\n",
      "labels.shape: (65392, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f'targets.shape: {targets.shape}')\n",
    "print(f'contexts.shape: {contexts.shape}')\n",
    "print(f'labels.shape: {labels.shape}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "660937b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for el in dataset1:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4af433b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def times(n):\n",
    "    return lambda x: x * n\n",
    "double = times(2)\n",
    "double(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0f1dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65392,) (65392, 5) (65392, 5)\n"
     ]
    }
   ],
   "source": [
    "print(targets.shape,contexts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86600299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "num_ns = 4\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd93c86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5)), (1024, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "683a19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=1,\n",
    "                                                 name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=num_ns+1)\n",
    "    \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        #target: (batch, dummy?) #the dummy axis doesn't exists in TF2.7+\n",
    "        #context :(batch, context)\n",
    "        # target: (batch,)\n",
    "        if len(target.shape) == 2:\n",
    "            print('len(target.shape) == 2')\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # word_emb: (batch, embed)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        print(f'word_emb{word_emb}')\n",
    "        # context_emb: (batch, context, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "\n",
    "        #dots: (batch, context)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        \n",
    "        return dots\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be71ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def custom_loss(x_logit, y_true):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2143cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "359f3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 18:48:28.437588: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-01-26 18:48:28.437627: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-01-26 18:48:28.438069: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3374d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_embedding = layers.Embedding(10,10,\n",
    "                                                 input_length=1,\n",
    "                                                 name=\"w2v\")\n",
    "enc_1  = target_embedding(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48f8ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2 = target_embedding(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "402b9572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.04993533,  0.04133791, -0.01003022, -0.03616273,  0.01099525,\n",
       "        -0.00558387, -0.046495  ,  0.00755589,  0.04464337, -0.04282141],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.04993533,  0.04133791, -0.01003022, -0.03616273,  0.01099525,\n",
       "        -0.00558387, -0.046495  ,  0.00755589,  0.04464337, -0.04282141],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_1,enc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8379172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "len(target.shape) == 2\n",
      "word_embTensor(\"word2_vec/w2v_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 128), dtype=float32)\n",
      "len(target.shape) == 2\n",
      "word_embTensor(\"word2_vec/w2v_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 128), dtype=float32)\n",
      " 9/63 [===>..........................] - ETA: 1s - loss: 1.6094 - accuracy: 0.2063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 18:48:29.408103: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-01-26 18:48:29.408132: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-01-26 18:48:29.429178: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-01-26 18:48:29.445056: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2023-01-26 18:48:29.463546: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/train/plugins/profile/2023_01_26_18_48_29\n",
      "\n",
      "2023-01-26 18:48:29.472960: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.trace.json.gz\n",
      "2023-01-26 18:48:29.482829: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/train/plugins/profile/2023_01_26_18_48_29\n",
      "\n",
      "2023-01-26 18:48:29.483681: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.memory_profile.json.gz\n",
      "2023-01-26 18:48:29.485486: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/train/plugins/profile/2023_01_26_18_48_29\n",
      "Dumped tool data for xplane.pb to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/train/plugins/profile/2023_01_26_18_48_29/neo.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 19ms/step - loss: 1.6083 - accuracy: 0.2292\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.5894 - accuracy: 0.5604\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5428 - accuracy: 0.6182\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.4601 - accuracy: 0.5887\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.3602 - accuracy: 0.5900\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.2615 - accuracy: 0.6145 0s - los\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1702 - accuracy: 0.6460\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.0860 - accuracy: 0.6812\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.0081 - accuracy: 0.7132\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9357 - accuracy: 0.7425\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8686 - accuracy: 0.7687\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.8065 - accuracy: 0.7906\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.7494 - accuracy: 0.8090\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6969 - accuracy: 0.8267\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.6489 - accuracy: 0.8418\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.6051 - accuracy: 0.8549\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.5651 - accuracy: 0.8674\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.5288 - accuracy: 0.8783\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.4957 - accuracy: 0.8878\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.4657 - accuracy: 0.8973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e6346d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72dc7aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-74d9ae2222f237dc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-74d9ae2222f237dc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e19857eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.04869585, -0.00979846,  0.01616162, ...,  0.04642085,\n",
       "         -0.01661394,  0.02395758],\n",
       "        [-0.18934697,  0.06779505, -0.00595527, ..., -0.15223621,\n",
       "          0.31419426,  0.02651004],\n",
       "        [ 0.05358925,  0.15102428, -0.02173385, ...,  0.06489161,\n",
       "         -0.20461024, -0.20701258],\n",
       "        ...,\n",
       "        [-0.17118086,  0.14345002,  0.04594244, ..., -0.00836325,\n",
       "          0.08011571, -0.18411285],\n",
       "        [-0.2470236 , -0.00500658, -0.22775547, ..., -0.19098292,\n",
       "         -0.04500531,  0.30172408],\n",
       "        [-0.03835887,  0.04480712,  0.20331377, ...,  0.27478793,\n",
       "          0.15031911,  0.08332097]], dtype=float32),\n",
       " ['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "weights, vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c9859d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbfe7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the vectors and metadata files:\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d91c2",
   "metadata": {},
   "source": [
    "#### Some resources that filled in the gaps for me \n",
    "Word2Vec TensorFlow Tutorial Walkthrough by Ya Xiao - https://www.youtube.com/watch?v=dxTwaUveedo\n",
    "<br>Making Computation Easier with Cool Numpy Tricks by Kirit Thadaka  - https://www.youtube.com/watch?v=poD8ud4MxOY\n",
    "<br> And the original tensorflow tutorial itself: https://www.tensorflow.org/tutorials/text/word2vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf775a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
